{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2476686,"sourceType":"datasetVersion","datasetId":1498861}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **演習：AI のバイアスを特定する**","metadata":{}},{"cell_type":"markdown","source":"**This notebook is an exercise in the [AI Ethics](https://www.kaggle.com/learn/ai-ethics) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/identifying-bias-in-ai).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"**　このノートブックは、[AI倫理](https://www.kaggle.com/learn/ai-ethics) コースの演習です。チュートリアルは、[このリンク](https://www.kaggle.com/var0101/human-centered-design-for-ai)** で参照できます。\n\n---","metadata":{}},{"cell_type":"markdown","source":"In the tutorial, you learned about six different types of bias.  In this exercise, you'll train a model with **real data** and get practice with identifying bias.  Don't worry if you're new to coding: you'll still be able to complete the exercise!\n\n# Introduction\n\nAt the end of 2017, the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and released their ~2 million public comments in a lasting open archive. Jigsaw sponsored this effort and helped to comprehensively annotate the data.  In 2019, Kaggle held the [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition so that data scientists worldwide could work together to investigate ways to mitigate bias.\n\nThe code cell below loads some of the data from the competition.  We'll work with thousands of comments, where each comment is labeled as either \"toxic\" or \"not toxic\".\n\nBegin by running the next code cell.  \n- Clicking inside the code cell.\n- Click on the triangle (in the shape of a \"Play button\") that appears to the left of the code cell.\n\nThe code will run for approximately 30 seconds.  When it finishes, you should see as output a message saying that the data was successfully loaded, along with two examples of comments: one is toxic, and the other is not.","metadata":{"papermill":{"duration":0.014195,"end_time":"2020-10-30T19:06:15.040067","exception":false,"start_time":"2020-10-30T19:06:15.025872","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"チュートリアルでは、6種類のバイアスについて学習しました。この演習では、**実際のデータ**を使用してモデルをトレーニングし、バイアスを識別する練習を行います。コーディングに慣れていなくても、演習を完了することができます。\n\n# はじめに\n\n2017年の終わりに、[市民コメント](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d)プラットフォームが閉鎖され、200万件のパブリックコメントが永続的なオープンアーカイブとして公開された。Jigsawはこの取り組みを支援し、データの包括的なアノテーション付けを支援した。2019年、Kaggleは[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview)コンテストを開催し、世界中のデータサイエンティストが協力してバイアスを軽減する方法を調査できるようにした。\n\n下のコードセルは、コンペのデータの一部を読み込みます。数千のコメントを処理し、各コメントに\"TOXIC(有害)\"または\"NOT TOXIC(無害)\"のラベルを付けます。\n\nまず、次のコードセルを実行します。\n- コードセル内をクリックします。\n- コードセルの左側に表示される三角形(\"再生ボタン\"の形)をクリックします。\n\nコードは約30秒間実行されます。完了すると、データが正常に読み込まれたことを示すメッセージと、2つのコメントの例(1つは有害、もう1つは無害) が出力されます。","metadata":{}},{"cell_type":"code","source":"# Set up feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ethics.ex3 import *\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Get the same results each time\nnp.random.seed(0)\n\n# Load the training data\ndata = pd.read_csv(\"../input/jigsaw-snapshot/data.csv\")\ncomments = data[\"comment_text\"]\ntarget = (data[\"target\"]>0.7).astype(int)\n\n# Break into training and test sets\ncomments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n\n# トレーニングデータから語彙の取得\nvectorizer = CountVectorizer()\nvectorizer.fit(comments_train)\n\n# Get word counts for training and test sets\nX_train = vectorizer.transform(comments_train)\nX_test = vectorizer.transform(comments_test)\n\n# Preview the dataset\nprint(\"Data successfully loaded!\\n\")\nprint(\"Sample toxic comment:\", comments_train.iloc[22])\nprint(\"Sample not-toxic comment:\", comments_train.iloc[17])","metadata":{"papermill":{"duration":30.692706,"end_time":"2020-10-30T19:06:45.747191","exception":false,"start_time":"2020-10-30T19:06:15.054485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T10:37:00.383996Z","iopub.execute_input":"2024-04-25T10:37:00.384385Z","iopub.status.idle":"2024-04-25T10:37:13.790519Z","shell.execute_reply.started":"2024-04-25T10:37:00.384356Z","shell.execute_reply":"2024-04-25T10:37:13.789290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Data successfully loaded!\n\nSample toxic comment: Too dumb to even answer.\nSample not-toxic comment: No they aren't.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"データの読み込みに成功しました!\n\n有害なコメントの例:頭が悪すぎて答えられない。  \n無害なコメントの例:いいえ、そうではありません。","metadata":{}},{"cell_type":"markdown","source":"Run the next code cell without changes to use the data to train a simple model.  The output shows the accuracy of the model on some test data.","metadata":{"papermill":{"duration":0.012727,"end_time":"2020-10-30T19:06:45.774334","exception":false,"start_time":"2020-10-30T19:06:45.761607","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n次のコードセルを変更せずに実行し、データを使用して単純なモデルをトレーニングします。  \n出力は、一部のテストデータに対するモデルの精度を示します。","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# モデルをトレーニングし、テストデータセットでパフォーマンスを評価する\nclassifier = LogisticRegression(max_iter=2000)\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\nprint(\"Accuracy:\", score)\n\n# 任意の文字列を分類する関数\ndef classify_string(string, investigate=False):\n    prediction = classifier.predict(vectorizer.transform([string]))[0]\n    if prediction == 0:\n        print(\"NOT TOXIC:\", string)\n    else:\n        print(\"TOXIC:\", string)","metadata":{"papermill":{"duration":11.047454,"end_time":"2020-10-30T19:06:56.836704","exception":false,"start_time":"2020-10-30T19:06:45.78925","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T10:41:19.978447Z","iopub.execute_input":"2024-04-25T10:41:19.978858Z","iopub.status.idle":"2024-04-25T10:41:48.746729Z","shell.execute_reply.started":"2024-04-25T10:41:19.978828Z","shell.execute_reply":"2024-04-25T10:41:48.744691Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Accuracy: 0.9304755967877966\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Roughly 93% of the comments in the test data are classified correctly!\n\n# 1) Try out the model\n\nYou'll use the next code cell to write your own comments and supply them to the model: does the model classify them as toxic?  \n\n1. Begin by running the code cell as-is to classify the comment `\"I love apples\"`.  You should see that was classified as \"NOT TOXIC\".\n\n2. Then, try out another comment: `\"Apples are stupid\"`.  To do this, change only `\"I love apples\"` and leaving the rest of the code as-is.  Make sure that your comment is enclosed in quotes, as below.\n```python\nmy_comment = \"Apples are stupid\"\n```\n3. Try out several comments (not necessarily about apples!), to see how the model performs: does it perform as suspected?","metadata":{"papermill":{"duration":0.014849,"end_time":"2020-10-30T19:06:56.868884","exception":false,"start_time":"2020-10-30T19:06:56.854035","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"テストデータのコメントの約93%が正しく分類されています。\n\n# 1) モデルを試す\n\n次のコードセルを使用して独自のコメントを記述し、モデルに提供します。モデルはコメントを有害として分類しますか。\n\n1. まず、コードセルをそのまま実行して、コメント`\"リンゴはすきです\"`を分類します。あなたはそれが「NOT TOXIC(無害)」 と分類されていることが分かるはずです。\n\n2. それから、別のコメントを試してみてください:「リンゴは愚かだ」。これを行うには、`\"リンゴはすきです\"`だけを変更し、残りのコードはそのままにします。コメントは、次のように引用符で囲んでください。\n```python\nmy_comment = \"リンゴは愚かだ\"\n```\n3. いくつかのコメント(必ずしもリンゴのことではありません!)を試して、モデルのパフォーマンスを確認します。","metadata":{}},{"cell_type":"code","source":"# Comment to pass through the model\nmy_comment = \"I love apples\"\n\n# Do not change the code below\nclassify_string(my_comment)\nq_1.check()","metadata":{"papermill":{"duration":0.026189,"end_time":"2020-10-30T19:06:56.90901","exception":false,"start_time":"2020-10-30T19:06:56.882821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T10:51:36.222419Z","iopub.execute_input":"2024-04-25T10:51:36.222834Z","iopub.status.idle":"2024-04-25T10:51:36.234443Z","shell.execute_reply.started":"2024-04-25T10:51:36.222803Z","shell.execute_reply":"2024-04-25T10:51:36.233163Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"NOT TOXIC: I love apples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"1_TryOut\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"markdown","source":"NOT TOXIC(無害):I love apples","metadata":{}},{"cell_type":"markdown","source":"Once you're done with testing comments, we'll move on to understand how the model makes decisions.  Run the next code cell without changes.\n\nThe model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic.  The code cell outputs the ten words that are considered most toxic, along with their coefficients.  ","metadata":{"papermill":{"duration":0.018539,"end_time":"2020-10-30T19:06:56.942426","exception":false,"start_time":"2020-10-30T19:06:56.923887","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"コメントのテストが完了したら、モデルがどのように決定を行うかを理解します。次のコードセルを変更せずに実行します。\n\nこのモデルでは、約58,000の単語それぞれに係数を割り当てており、係数が高いほど毒性が高いとモデルが判断した単語を表します。コードセルは、最も有害と見なされる10個の単語とその係数を出力します。","metadata":{}},{"cell_type":"code","source":"coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\ncoefficients.sort_values(by=['coeff']).tail(10)","metadata":{"papermill":{"duration":0.115908,"end_time":"2020-10-30T19:06:57.07637","exception":false,"start_time":"2020-10-30T19:06:56.960462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T10:53:19.218919Z","iopub.execute_input":"2024-04-25T10:53:19.220204Z","iopub.status.idle":"2024-04-25T10:53:19.298889Z","shell.execute_reply.started":"2024-04-25T10:53:19.220160Z","shell.execute_reply":"2024-04-25T10:53:19.297722Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            word     coeff\n20745      fools  6.279304\n34211      moron  6.332672\n16844       dumb  6.359549\n12907       crap  6.489868\n38317   pathetic  6.554616\n25850    idiotic  7.005480\n49802  stupidity  7.554089\n25858     idiots  8.602875\n25847      idiot  8.606217\n49789     stupid  9.279511","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>coeff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20745</th>\n      <td>fools</td>\n      <td>6.279304</td>\n    </tr>\n    <tr>\n      <th>34211</th>\n      <td>moron</td>\n      <td>6.332672</td>\n    </tr>\n    <tr>\n      <th>16844</th>\n      <td>dumb</td>\n      <td>6.359549</td>\n    </tr>\n    <tr>\n      <th>12907</th>\n      <td>crap</td>\n      <td>6.489868</td>\n    </tr>\n    <tr>\n      <th>38317</th>\n      <td>pathetic</td>\n      <td>6.554616</td>\n    </tr>\n    <tr>\n      <th>25850</th>\n      <td>idiotic</td>\n      <td>7.005480</td>\n    </tr>\n    <tr>\n      <th>49802</th>\n      <td>stupidity</td>\n      <td>7.554089</td>\n    </tr>\n    <tr>\n      <th>25858</th>\n      <td>idiots</td>\n      <td>8.602875</td>\n    </tr>\n    <tr>\n      <th>25847</th>\n      <td>idiot</td>\n      <td>8.606217</td>\n    </tr>\n    <tr>\n      <th>49789</th>\n      <td>stupid</td>\n      <td>9.279511</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2) Most toxic words\n\nTake a look at the most toxic words from the code cell above.  Are you surprised to see any of them?  Are there any words that seem like they should not be in the list?","metadata":{"papermill":{"duration":0.015335,"end_time":"2020-10-30T19:06:57.106908","exception":false,"start_time":"2020-10-30T19:06:57.091573","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 2) 最も有害な言葉\n\n上のコードセルから最も有害な単語を見てみましょう。どれを見ても驚きませんか?  \nリストに入れてはいけないような言葉はありますか?","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_2.check()","metadata":{"papermill":{"duration":0.022976,"end_time":"2020-10-30T19:06:57.145202","exception":false,"start_time":"2020-10-30T19:06:57.122226","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T10:54:47.496691Z","iopub.execute_input":"2024-04-25T10:54:47.497063Z","iopub.status.idle":"2024-04-25T10:54:47.506054Z","shell.execute_reply.started":"2024-04-25T10:54:47.497033Z","shell.execute_reply":"2024-04-25T10:54:47.504788Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"2_MostToxic\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: None of the words are surprising.  They are all clearly toxic.","text/markdown":"<span style=\"color:#33cc33\">Solution:</span> None of the words are surprising.  They are all clearly toxic."},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"color:#33cc33\">解決策:</span>どれも驚くような言葉ではありません。それらはすべて明らかに有毒である。","metadata":{}},{"cell_type":"markdown","source":"# 3) A closer investigation\n\nWe'll take a closer look at how the model classifies comments.\n1. Begin by running the code cell as-is to classify the comment `\"I have a christian friend\"`.  You should see that was classified as \"NOT TOXIC\".  In addition, you can see what scores were assigned to some of the individual words.  Note that all words in the comment likely won't appear.\n2. Next, try out another comment: `\"I have a muslim friend\"`.  To do this, change only `\"I have a christian friend\"` and leave the rest of the code as-is. Make sure that your comment is enclosed in quotes, as below.\n```python\nnew_comment = \"I have a muslim friend\"\n```\n3. Try out two more comments: `\"I have a white friend\"` and `\"I have a black friend\"` (in each case, do not add punctuation to the comment).\n4. Feel free to try out more comments, to see how the model classifies them.","metadata":{"papermill":{"duration":0.015333,"end_time":"2020-10-30T19:06:57.175887","exception":false,"start_time":"2020-10-30T19:06:57.160554","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 3) 詳細な調査\n\nモデルがコメントを分類する方法について詳しく説明します。\n1. まず、コードセルをそのまま実行して、コメント`\"I have a christian friend（キリスト教徒の友人がいます）\"`を分類します。あなたはそれが「NOT TOXIC(無害)」に分類されているのを見るはずです。さらに、個々の単語に割り当てられたスコアを確認できます。コメント内のすべての単語が表示されるわけではないことに注意してください。\n2. 次に、\"I have a muslim friend(イスラム教徒の友人がいる)\"という別のコメントを試してみてください。これを行うには、`\"I have a christian friend（キリスト教徒の友人がいます）\"`だけを変更し、残りのコードはそのままにします。コメントは、次のように引用符で囲んでください。\n```python\nnew_comment=\"I have a muslim friend(イスラム教徒の友人がいる)\"\n```\n3. \"I have a white friend（私には白人の友達がいる）\"と\"I have a black friend（私には黒人の友達がいる）\"という2つのコメントも試してみてください(いずれの場合も、コメントに句読点を追加しないでください。)。\n4. さらにコメントを試してみて、モデルによるコメントの分類方法を確認してください。","metadata":{}},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment = \"I have a christian friend\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\nq_3.check()","metadata":{"papermill":{"duration":0.044861,"end_time":"2020-10-30T19:06:57.236559","exception":false,"start_time":"2020-10-30T19:06:57.191698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T11:05:57.492827Z","iopub.execute_input":"2024-04-25T11:05:57.493954Z","iopub.status.idle":"2024-04-25T11:05:57.517755Z","shell.execute_reply.started":"2024-04-25T11:05:57.493914Z","shell.execute_reply":"2024-04-25T11:05:57.516674Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"NOT TOXIC: I have a christian friend\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"3_CloserInvestigation\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment=\"I have a muslim friend(イスラム教徒の友人がいる)\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\nq_3.check()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:09:04.462498Z","iopub.execute_input":"2024-04-25T11:09:04.462928Z","iopub.status.idle":"2024-04-25T11:09:04.483006Z","shell.execute_reply.started":"2024-04-25T11:09:04.462896Z","shell.execute_reply":"2024-04-25T11:09:04.481786Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"TOXIC: I have a muslim friend(イスラム教徒の友人がいる)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"3_CloserInvestigation\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment=\"I have a white friend（私には白人の友達がいる）\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\nq_3.check()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:10:18.632109Z","iopub.execute_input":"2024-04-25T11:10:18.632737Z","iopub.status.idle":"2024-04-25T11:10:18.651334Z","shell.execute_reply.started":"2024-04-25T11:10:18.632707Z","shell.execute_reply":"2024-04-25T11:10:18.650420Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"NOT TOXIC: I have a white friend（私には白人の友達がいる）\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"3_CloserInvestigation\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment=\"I have a black friend（私には黒人の友達がいる）\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\nq_3.check()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:10:38.147848Z","iopub.execute_input":"2024-04-25T11:10:38.148687Z","iopub.status.idle":"2024-04-25T11:10:38.170653Z","shell.execute_reply.started":"2024-04-25T11:10:38.148653Z","shell.execute_reply":"2024-04-25T11:10:38.169485Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"TOXIC: I have a black friend（私には黒人の友達がいる）\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"3_CloserInvestigation\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4) Identify bias\n\nDo you see any signs of potential bias in the model?  In the code cell above,\n- How did the model classify `\"I have a christian friend\"` and `\"I have a muslim friend\"`?  \n- How did it classify `\"I have a white friend\"` and `\"I have a black friend\"`?    \n\nOnce you have an answer, run the next code cell.","metadata":{"papermill":{"duration":0.016152,"end_time":"2020-10-30T19:06:57.26916","exception":false,"start_time":"2020-10-30T19:06:57.253008","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 4) バイアスの特定\n\nモデルに潜在的なバイアスの兆候はありますか?上のコードセルで、\n- モデルは「私にはキリスト教徒の友人がいる」と「私にはイスラム教徒の友人がいる」をどのように分類したのか?\n- 「私には白人の友人がいる」と「私には黒人の友人がいる」はどのように分類されたのか?\n\n答えが見つかったら、次のコードセルを実行します。","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_4.check()","metadata":{"papermill":{"duration":0.024534,"end_time":"2020-10-30T19:06:57.310158","exception":false,"start_time":"2020-10-30T19:06:57.285624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T11:06:30.095954Z","iopub.execute_input":"2024-04-25T11:06:30.096920Z","iopub.status.idle":"2024-04-25T11:06:30.105111Z","shell.execute_reply.started":"2024-04-25T11:06:30.096884Z","shell.execute_reply":"2024-04-25T11:06:30.104007Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"4_IdentifyBias\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: The comment `I have a muslim friend` was marked as toxic, whereas `I have a christian friend` was not.  Likewise, `I have a black friend` was marked as toxic, whereas `I have a white friend` was not.  None of these comments should be marked as toxic, but the model seems to erroneously associate some identities as toxic.  This is a sign of bias: the model seems biased in favor of `christian` and against `muslim`, and it seems biased in favor of `white` and against `black`.","text/markdown":"<span style=\"color:#33cc33\">Solution:</span> The comment `I have a muslim friend` was marked as toxic, whereas `I have a christian friend` was not.  Likewise, `I have a black friend` was marked as toxic, whereas `I have a white friend` was not.  None of these comments should be marked as toxic, but the model seems to erroneously associate some identities as toxic.  This is a sign of bias: the model seems biased in favor of `christian` and against `muslim`, and it seems biased in favor of `white` and against `black`."},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"color:#33cc33\">解決策:</span>私にはイスラム教徒の友人がいるというコメントは有害とマークされたが、私にはキリスト教徒の友人がいるというコメントは有害ではなかった。同様に、私は黒人の友人が有毒とマークされていたのに対し、私は白人の友人がそうではありませんでした。これらのコメントはいずれも有害とマークされるべきではないが、モデルは一部のアイデンティティを有害と誤って関連付けているようである。これはバイアスの兆候です。このモデルは、キリスト教徒を支持し、イスラム教徒に対してバイアスがかかっているように見えます。また、白人を支持し、黒人に対してバイアスがかかっているように見えます。\n","metadata":{}},{"cell_type":"markdown","source":"# 5) Test your understanding\n\nWe'll step away from the Jigsaw competition data and consider a similar (but hypothetical!) scenario where you're working with a dataset of online comments to train a model to classify comments as toxic.\n\nYou notice that comments that refer to Islam are more likely to be toxic than comments that refer to other religions, because the online community is islamophobic.  What type of bias can this introduce to your model?\n\nOnce you have answered the question, run the next code cell to see the official answer.","metadata":{"papermill":{"duration":0.016002,"end_time":"2020-10-30T19:06:57.342755","exception":false,"start_time":"2020-10-30T19:06:57.326753","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 5) 理解度をテストする\n\nここではJigsawの競合データから離れて、同様の(しかし、仮定の話です!)シナリオについて考えます。このシナリオでは、オンラインコメントのデータセットを使用して、コメントを有害として分類するようにモデルをトレーニングします。\n\nイスラム教に言及するコメントは、他の宗教に言及するコメントよりも有害である可能性が高いことがわかります。これは、オンラインコミュニティがイスラム恐怖症であるためです。これにより、モデルにどのような種類のバイアスが発生する可能性がありますか。\n\n質問に答えたら、次のコードセルを実行して正式な回答を確認します。","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_5.check()","metadata":{"papermill":{"duration":0.025022,"end_time":"2020-10-30T19:06:57.384809","exception":false,"start_time":"2020-10-30T19:06:57.359787","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-25T11:13:22.234590Z","iopub.execute_input":"2024-04-25T11:13:22.234997Z","iopub.status.idle":"2024-04-25T11:13:22.244210Z","shell.execute_reply.started":"2024-04-25T11:13:22.234967Z","shell.execute_reply":"2024-04-25T11:13:22.243140Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"5_TestUnderstanding\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: Comments that refer to Islam are more likely to be classified as toxic, because of a flawed state of the online community where the data was collected.  This can introduce **historical bias**.","text/markdown":"<span style=\"color:#33cc33\">Solution:</span> Comments that refer to Islam are more likely to be classified as toxic, because of a flawed state of the online community where the data was collected.  This can introduce **historical bias**."},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"color:#33cc33\">解決策:</span>イスラム教に言及したコメントは、データが収集されたオンラインコミュニティの状態に欠陥があるため、有害と分類される可能性が高い。これは**歴史的バイアス**をもたらす可能性がある。\n","metadata":{}},{"cell_type":"markdown","source":"# 6) Test your understanding, part 2\n\nWe'll continue with the same hypothetical scenario, where you're trying to train a model to classify online comments as toxic.\n\nYou take any comments that are not already in English and translate them to English with a separate tool.  Then, you treat all posts as if they were originally expressed in English.  What type of bias will your model suffer from?\n\nOnce you have answered the question, run the next code cell to see the official answer.","metadata":{}},{"cell_type":"markdown","source":"# 6) 理解度テスト、パート2\n\n引き続き、オンラインコメントを有害として分類するようにモデルをトレーニングしようとしている、同じ仮説シナリオについて説明します。\n\n英語になっていないコメントは、別のツールで英語に翻訳します。次に、すべての投稿を元は英語で表現されたものとして扱います。モデルはどのような種類のバイアスに悩まされるのでしょうか。\n\n質問に答えたら、次のコードセルを実行して正式な回答を確認します。","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_6.check()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:14:48.937240Z","iopub.execute_input":"2024-04-25T11:14:48.938126Z","iopub.status.idle":"2024-04-25T11:14:48.947383Z","shell.execute_reply.started":"2024-04-25T11:14:48.938060Z","shell.execute_reply":"2024-04-25T11:14:48.946143Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"6_TestUnderstandingTwo\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: By translating comments to English, we introduce additional error when classifying non-English comments.  This can introduce **measurement bias**, since non-English comments will often not be translated perfectly.  It could also introduce **aggregation bias**: the model would likely perform better for comments expressed in all languages, if the comments from different languages were treated differently.","text/markdown":"<span style=\"color:#33cc33\">Solution:</span> By translating comments to English, we introduce additional error when classifying non-English comments.  This can introduce **measurement bias**, since non-English comments will often not be translated perfectly.  It could also introduce **aggregation bias**: the model would likely perform better for comments expressed in all languages, if the comments from different languages were treated differently."},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"color:#33cc33\">解決策:</span>コメントを英語に翻訳することにより、非英語コメントを分類するときに追加のエラーを導入した。英語以外のコメントはしばしば完璧に翻訳されないので、これは**測定バイアス**をもたらす可能性があります。また、**集計バイアス**が発生する可能性もあります。異なる言語のコメントを異なる方法で処理すると、すべての言語で表現されたコメントに対してモデルのパフォーマンスが向上する可能性があります。\n","metadata":{}},{"cell_type":"markdown","source":"# 7) Test your understanding, part 3\n\nWe'll continue with the same hypothetical scenario, where you're trying to train a model to classify online comments as toxic.\n\nThe dataset you're using to train the model contains comments primarily from users based in the United Kingdom.  \n\nAfter training a model, you evaluate its performance with another dataset of comments, also primarily from users based in the United Kingdom -- and it gets great performance!  You deploy it for a company based in Australia, and it does not perform well, because of differences between British and Australian English.  What types of bias does the model suffer from?\n\nOnce you have answered the question, run the next code cell to see the official answer.","metadata":{}},{"cell_type":"markdown","source":"# 7) 理解度テスト、パート3\n\n引き続き、オンラインコメントを有害として分類するようにモデルをトレーニングしようとしている、同じ仮説シナリオについて説明します。\n\nモデルのトレーニングに使用しているデータセットには、主に英国を拠点とするユーザーからのコメントが含まれています。\n\nモデルをトレーニングした後、別のコメントデータセットを使用してそのパフォーマンスを評価します。これも主に英国を拠点とするユーザーからのコメントです。オーストラリアに拠点を置く会社のためにそれを展開し、英国英語とオーストラリア英語の違いのためにうまく機能しません。モデルはどのような種類のバイアスを受けているか。\n\n質問に答えたら、次のコードセルを実行して正式な回答を確認します。","metadata":{}},{"cell_type":"code","source":"# Check your answer (Run this code cell to get credit!)\nq_7.check()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:18:28.870426Z","iopub.execute_input":"2024-04-25T11:18:28.870842Z","iopub.status.idle":"2024-04-25T11:18:28.880862Z","shell.execute_reply.started":"2024-04-25T11:18:28.870811Z","shell.execute_reply":"2024-04-25T11:18:28.879670Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"7_TestUnderstandingThree\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: If the model is evaluated based on comments from users in the United Kingdom and deployed to users in Australia, this will lead to **evaluation bias** and **deployment bias**.  The model will also have **representation bias**, because it was built to serve users in Australia, but was trained with data from users based in the United Kingdom.","text/markdown":"<span style=\"color:#33cc33\">Solution:</span> If the model is evaluated based on comments from users in the United Kingdom and deployed to users in Australia, this will lead to **evaluation bias** and **deployment bias**.  The model will also have **representation bias**, because it was built to serve users in Australia, but was trained with data from users based in the United Kingdom."},"metadata":{}}]},{"cell_type":"markdown","source":"<span style=\"color:#33cc33\">解決策:</span>英国のユーザーからのコメントに基づいてモデルを評価し、オーストラリアのユーザーに展開すると、**評価バイアス**と**展開バイアス**が発生します。このモデルはオーストラリアのユーザーにサービスを提供するように構築されていますが、英国を拠点とするユーザーのデータでトレーニングされているため、**表現バイアス**もあります。\n","metadata":{}},{"cell_type":"markdown","source":"# Learn more\n\nTo continue learning about bias, check out the [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition that was introduced in this exercise.  \n- Kaggler [Dieter](https://www.kaggle.com/christofhenkel) has written a helpful two-part series that teaches you how to preprocess the data and train a neural network to make a competition submission.  [Get started here](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda).\n- Many Kagglers have written helpful notebooks that you can use to get started.  [Check them out](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/notebooks?sortBy=voteCount&group=everyone&pageSize=20&competitionId=12500) on the competition page.\n\nAnother Kaggle competition that you can use to learn about bias is the [Inclusive Images Challenge](https://www.kaggle.com/c/inclusive-images-challenge), which you can read more about in [this blog post](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html).  The competition focuses on **evaluation bias** in computer vision.\n\n# Keep going\n\nHow can you quantify bias in machine learning applications?  Continue to **[learn how to measure fairness](https://www.kaggle.com/alexisbcook/ai-fairness)**.","metadata":{"papermill":{"duration":0.016326,"end_time":"2020-10-30T19:06:57.417832","exception":false,"start_time":"2020-10-30T19:06:57.401506","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 詳細\n\nバイアスの学習を続けるには、この演習で紹介した[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview)コンペティションを参照してください。\n-Kaggler[Dieter](https://www.kaggle.com/christofhenkel)は、データを前処理し、ニューラルネットワークをトレーニングしてコンペに提出する方法を説明する役立つ2部構成のシリーズを執筆しました。[ご利用開始はこちら](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda)。\n-多くのKagglerが役に立つノートを書いているので、それを使って始めることができます。[Check them out](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/notebooks?sortBy=voteCount&group=everyone&pageSize=20&competitionId=12500)大会ページ。\n\nバイアスについて学ぶために使用できるもう1つのKaggleコンテストは、[Inclusive Images Challenge](https://www.kaggle.com/c/inclusive-images-challenge)で、詳細については[このブログ記事](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html)を参照してください。このコンテストは、コンピュータービジョンにおける**評価バイアス**に焦点を当てています。\n\n# 続けて\n\n機械学習アプリケーションのバイアスを定量化するにはどうすればよいでしょうか。引き続き**[公平性を測る方法を学ぶ](https://www.kaggle.com/alexisbcook/ai-fairness)**。","metadata":{}}]}