{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092ad639",
   "metadata": {
    "papermill": {
     "duration": 0.003232,
     "end_time": "2024-04-25T18:14:58.848826",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.845594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **チュートリアル：AI のバイアスを特定する**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d5a99",
   "metadata": {
    "papermill": {
     "duration": 0.002696,
     "end_time": "2024-04-25T18:14:58.854843",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.852147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning(ML) has the potential to improve lives, but it can also be a source of harm. ML applications have discriminated against individuals on the basis of race, sex, religion, socioeconomic status, and other categories.\n",
    "\n",
    "In this tutorial, you’ll learn about **bias**, which refers to negative, unwanted consequences of ML applications, especially if the consequences disproportionately affect certain groups.\n",
    "\n",
    "We’ll cover six different types of bias that can affect any ML application.  Then you’ll put your new knowledge to work in a **[hands-on exercise](https://www.kaggle.com/kernels/fork/15622654)**, where you will identify bias in a real-world scenario.\n",
    "\n",
    "# Bias is complex\n",
    "\n",
    "Many ML practitioners are familiar with “biased data” and the concept of “garbage in, garbage out”. For example, if you’re training a chatbot using a dataset containing anti-Semitic online conversations(“garbage in”), the chatbot will likely make anti-Semitic remarks(“garbage out”).  This example details an important type of bias(called **historial bias**, as you’ll see below) that should be recognized and addressed.\n",
    "\n",
    "This is not the only way that bias can ruin ML applications. \n",
    "\n",
    "Bias in data is complex.  Flawed data can also result in **representation bias**(covered later in this tutorial), if a group is underrepresented in the training data.  For instance, when training a facial detection system, if the training data contains mostly individuals with lighter skin tones, it will fail to perform well for users with darker skin tones.  A third type of bias that can arise from the training data is called **measurement bias**, which you’ll learn about below. \n",
    "\n",
    "And it’s not just biased data that can lead to unfair ML applications: as you’ll learn, bias can also result from the way in which the ML model is defined, from the way the model is compared to other models, and from the way that everyday users interpret the final results of the model.  Harm can come from anywhere in the ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b32b3",
   "metadata": {
    "papermill": {
     "duration": 0.002615,
     "end_time": "2024-04-25T18:14:58.860587",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.857972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# はじめに\n",
    "\n",
    "機械学習(ML)は人々の生活を向上させる可能性を秘めているが、害をもたらす可能性もある。MLアプリケーションは、人種、性別、宗教、社会経済的地位、その他のカテゴリーに基づいて個人を差別してきた。\n",
    "\n",
    "このチュートリアルでは、**バイアス**について学習します。これは、MLアプリケーションの望ましくない否定的な結果、特に特定のグループに偏った影響を与える場合を指します。\n",
    "\n",
    "ここでは、あらゆるMLアプリケーションに影響を与える可能性のある6種類のバイアスについて説明します。次に、**[実践演習](https://www.kaggle.com/kernels/fork/15622654)**で新しい知識を活用し、実際のシナリオでバイアスを特定します。\n",
    "\n",
    "# バイアスは複雑です\n",
    "\n",
    "MLの実践者の多くは、「ごみの出し入れ」という概念に精通している。たとえば、反ユダヤ的なオンライン会話を含むデータセットを使用してチャットボットをトレーニングしている場合(\"ガベージ・イン\")、チャットボットは反ユダヤ的な発言をする可能性が高い(\"ガベージ・アウト\")。この例は、認識して対処すべき重要なタイプのバイアス(**歴史的バイアス**と呼ばれる、以下のように)を詳述している。\n",
    "\n",
    "偏見がMLアプリケーションをダメにするのはこれだけではない。\n",
    "\n",
    "データの偏りは複雑だ。また、トレーニングデータでグループが過小評価されている場合、欠陥のあるデータによって**表現バイアス**(このチュートリアルの後半で説明)が発生する可能性もあります。たとえば、顔検出システムをトレーニングする場合、トレーニングデータに明るい肌の色の個人がほとんど含まれていると、暗い肌の色のユーザーに対してはうまく機能しません。トレーニングデータから発生する可能性がある3番目の種類のバイアスは、**測定バイアス**と呼ばれます。これについては、以下で説明します。\n",
    "\n",
    "また、不公平なMLアプリケーションにつながる可能性があるのは、バイアスがかかったデータだけではありません。学習するように、バイアスは、MLモデルの定義方法、モデルを他のモデルと比較する方法、および日常的なユーザーがモデルの最終結果を解釈する方法によっても発生する可能性があります。MLプロセスのどこからでも危害が発生する可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c45a1",
   "metadata": {
    "papermill": {
     "duration": 0.003043,
     "end_time": "2024-04-25T18:14:58.867025",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.863982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Six types of bias\n",
    "\n",
    "Once we’re aware of the different types of bias, we are more likely to detect them in ML projects.  Furthermore, with a common vocabulary, we can have fruitful conversations about how to mitigate(or reduce) the bias.\n",
    "\n",
    "We will closely follow a [research paper](https://arxiv.org/pdf/1901.10002.pdf) from early 2020 that characterizes six different types of bias.   \n",
    "\n",
    "## Historical bias \n",
    "\n",
    "**Historical bias** occurs when the state of the world in which the data was generated is flawed. \n",
    "\n",
    "> As of 2020, only [7.4%](https://edition.cnn.com/2020/05/20/us/fortune-500-women-ceos-trnd) of Fortune 500 CEOs are women.  Research has shown that companies with female CEOs or CFOs are generally [more profitable](https://edition.cnn.com/2019/10/16/success/women-ceos-and-cfos-outperform/index.html) than companies with men in the same position, suggesting that women are held to higher hiring standards than men.  In order to fix this, we might consider removing human input and using AI to make the hiring process more equitable.  But this can prove unproductive if data from past hiring decisions is used to train a model, because the model will likely learn to demonstrate the same biases that are present in the data.\n",
    "\n",
    "## Representation bias \n",
    "\n",
    "**Representation bias** occurs when building datasets for training a model, if those datasets poorly represent the people that the model will serve.\n",
    "\n",
    "> Data collected through smartphone apps will under-represent groups that are less likely to own smartphones.  For instance, if collecting [data in the USA](https://www.pewresearch.org/internet/fact-sheet/mobile/#:~:text=The%20vast%20majority%20of%20Americans,range%20of%20other%20information%20devices), individuals over the age of 65 will be under-represented.  If the data is used to inform design of a city transportation system, this will be disastrous, since older people have important [needs](https://www.bloomberg.com/news/articles/2017-08-04/why-aging-americans-need-better-transit) to ensure that the system is accessible.\n",
    "\n",
    "## Measurement bias\n",
    "\n",
    "**Measurement bias** occurs when the accuracy of the data varies across groups.  This can happen when working with proxy variables(variables that take the place of a variable that cannot be directly measured), if the quality of the proxy varies in different groups.\n",
    "\n",
    "> Your local hospital uses a model to identify high-risk patients before they develop serious conditions, based on information like past diagnoses, medications, and demographic data.  The model uses this information to predict health care costs, the idea being that patients with higher costs likely correspond to high-risk patients.  Despite the fact that the model specifically excludes race, it seems to demonstrate racial discrimination: the algorithm is less likely to select eligible Black patients.  How can this be the case?  It is because cost was used as a proxy for risk, and the relationship between these variables varies with race: Black patients experience increased barriers to care, have [less trust](https://science.sciencemag.org/content/366/6464/447) in the health care system, and therefore have lower medical costs, on average, when compared to non-Black patients with the same health conditions.\n",
    "\n",
    "## Aggregation bias \n",
    "\n",
    "**Aggregation bias** occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group or only performs well for the majority group. (This is often not an issue, but most commonly arises in medical applications.)\n",
    "\n",
    "> Hispanics have [higher rates](https://care.diabetesjournals.org/content/31/2/240.short) of diabetes and diabetes-related complications than non-Hispanic whites.  If building AI to diagnose or monitor diabetes, it is important to make the system sensitive to these ethnic differences, by either including ethnicity as a feature in the data, or building separate models for different ethnic groups.\n",
    "\n",
    "## Evaluation bias \n",
    "\n",
    "**Evaluation bias** occurs when evaluating a model, if the benchmark data(used to compare the model to other models that perform similar tasks) does not represent the population that the model will serve.\n",
    "\n",
    "> The [Gender Shades](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) paper discovered that two widely used facial analysis benchmark datasets(IJB-A and Adience) were primarily composed of lighter-skinned subjects(79.6% and 86.2%, respectively).  Commercial gender classification AI showed state-of-the-art performance on these benchmarks, but experienced disproportionately [high error rates](http://gendershades.org/overview.html) with people of color. \n",
    "\n",
    "## Deployment bias \n",
    "\n",
    "**Deployment bias** occurs when the problem the model is intended to solve is different from the way it is actually used.  If the end users don’t use the model in the way it is intended, there is no guarantee that the model will perform well.\n",
    "\n",
    "> The criminal justice system uses [tools](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/) to predict the likelihood that a convicted criminal will relapse into criminal behavior.  The predictions are [not designed for judges](https://onlinelibrary.wiley.com/doi/full/10.1002/bsl.2456) when deciding appropriate punishments at the time of sentencing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a0b65",
   "metadata": {
    "papermill": {
     "duration": 0.002822,
     "end_time": "2024-04-25T18:14:58.873504",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.870682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6種類のバイアス\n",
    "\n",
    "さまざまな種類のバイアスを認識すれば、MLプロジェクトでそれらを検出する可能性が高くなります。さらに、共通のボキャブラリーがあれば、バイアスを軽減する (または減らす) 方法について実りある会話をすることができます。\n",
    "\n",
    "我々は、六つの異なるタイプのバイアスを特徴付ける2020年初めの[研究論文](https://arxiv.org/pdf/1901.10002.pdf)を綿密に追跡する。\n",
    "\n",
    "## 歴史的バイアス\n",
    "\n",
    "**履歴バイアス**は、データが生成された世界の状態に欠陥がある場合に発生します。\n",
    "\n",
    "> 2020年のフォーチュン500社のCEOのうち、女性はわずか[7.4%](https://edition.cnn.com/2020/05/20/us/fortune-500-women-ceos-trnd) です。調査によると、女性のCEOやCFOがいる企業は、男性が同じ地位にいる企業よりも一般的に[収益性が高い](https://edition.cnn.com/2019/10/16/success/women-ceos-and-cfos-outperform/index.html)ことが示されており、女性は男性よりも採用基準が高いことを示唆しています。これを解決するために、人間の入力を排除し、AIを使用して雇用プロセスをより公平にすることを検討することができます。しかし、過去の採用決定のデータを使用してモデルをトレーニングする場合、モデルはデータに存在するのと同じバイアスを示すことを学習する可能性が高いため、これは非生産的であることがわかります。\n",
    "\n",
    "## 表現バイアス\n",
    "\n",
    "**表現バイアス**は、モデルをトレーニングするためのデータセットを構築するときに、それらのデータセットがモデルがサービスを提供するユーザーを適切に表していない場合に発生します。\n",
    "\n",
    "> スマートフォンアプリを通じて収集されたデータは、スマートフォンを所有する可能性が低いグループを過小評価することになる。たとえば[米国のデータ](https://www.pewresearch.org/internet/fact-sheet/mobile/#:~:text=The%20vast%20majority%20of%20Americans,range%20of%20other%20information%20devices)を収集する場合、65歳以上の個人は過小評価されます。もしこのデータが都市交通システムの設計に使われるとしたら、これは悲惨なことになるだろう。なぜなら、高齢者はシステムが利用しやすいことを保証する重要な[ニーズ](https://www.bloomberg.com/news/articles/2017-08-04/why-aging-americans-need-better-transit)を持っているからだ。\n",
    "\n",
    "## 測定バイアス\n",
    "\n",
    "**測定バイアス**は、データの精度がグループ間で異なる場合に発生します。これは、プロキシ変数(直接測定できない変数の代わりとなる変数)を操作するときに、プロキシの品質がグループによって異なる場合に発生する可能性があります。\n",
    "\n",
    "> 地元の病院では、過去の診断、投薬、人口統計データなどの情報に基づいて、重篤な状態になる前に高リスク患者を特定するモデルを使用しています。このモデルでは、この情報を使用して医療費を予測しており、医療費が高い患者は高リスク患者である可能性が高いという考えに基づいている。このモデルは人種を明確に除外しているにもかかわらず、人種差別を示しているように思われる。つまり、このアルゴリズムは適格な黒人患者を選択する可能性が低い。どうしてそうなるのでしょうか?これは、費用がリスクの代理変数として使用されており、これらの変数間の関係が人種によって異なるためです。黒人患者は、同じ健康状態の非黒人患者と比較して、医療への障壁が高く、医療制度への[信頼性の低下](https://science.sciencemag.org/content/366/6464/447)が低いため、平均して医療費が低くなります。\n",
    "\n",
    "## 集計バイアス\n",
    "\n",
    "**集計バイアス**は、グループの組み合わせが不適切な場合に発生します。その結果、モデルはどのグループに対しても適切に実行されないか、大多数のグループに対してのみ適切に実行されます。(多くの場合、これは問題ではありませんが、最も一般的には医療アプリケーションで発生します。)\n",
    "\n",
    "> ヒスパニックは非ヒスパニック系白人よりも糖尿病と糖尿病関連合併症の[率が高い](https://care.diabetesjournals.org/content/31/2/240.short)。糖尿病を診断または監視するAIを構築する場合は、データの特徴として民族を含めるか、民族グループごとに個別のモデルを構築することによって、システムをこれらの民族の違いに敏感にすることが重要である。\n",
    "\n",
    "## 評価バイアス\n",
    "\n",
    "**評価バイアス**は、モデルを評価するときに、ベンチマークデータ(モデルを同様のタスクを実行する他のモデルと比較するために使用されます)がモデルが提供する母集団を表していない場合に発生します。\n",
    "\n",
    ">[Gender Shades](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)の論文では、2つの広く使用されている顔分析ベンチマークデータセット(IJB-AとAdience)が、主に肌の色が明るい被験者(それぞれ79.6%と86.2%)で構成されていることがわかりました。商用の性別分類AIは、これらのベンチマークで最先端のパフォーマンスを示したが、有色人種では不釣り合いな[高いエラー率](http://gendershades.org/overview.html)を経験した。\n",
    "\n",
    "## 導入バイアス\n",
    "\n",
    "**配置バイアス**は、モデルが解決しようとしている問題が実際の使用方法と異なる場合に発生します。エンドユーザーが意図した方法でモデルを使用しない場合、モデルが適切に実行される保証はありません。\n",
    "\n",
    "> 刑事司法制度は、有罪判決を受けた犯罪者が犯罪行為に逆戻りする可能性を予測するために[ツール](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/)を使用しています。予測は、量刑時の量刑を決める際の[裁判官用ではない](https://onlinelibrary.wiley.com/doi/full/10.1002/bsl.2456) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c498e0",
   "metadata": {
    "papermill": {
     "duration": 0.002575,
     "end_time": "2024-04-25T18:14:58.878880",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.876305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can visually represent these different types of bias, which occur at different stages in the ML workflow:\n",
    "\n",
    "[![visual representation of types of bias](https://storage.googleapis.com/kaggle-media/learn/images/bvArGRY.png)](https://arxiv.org/pdf/1901.10002.pdf)\n",
    "\n",
    "Note that these are *not mutually exclusive*: that is, an ML application can easily suffer from more than one type of bias.  For example, as Rachel Thomas describes in a [recent research talk](https://www.youtube.com/watch?v=1Uyc9SPeYkA&list=PLe6zdIMe5B7IR0oDOobXBDBlYY1eqLYPx&index=10&t=41s), ML applications in wearable fitness devices can suffer from:\n",
    "- **Representation bias**(if the dataset used to train the models exclude darker skin tones), \n",
    "- **Measurement bias**(if the measurement apparatus shows reduced performance with dark skin tones), and \n",
    "- **Evaluation bias**(if the dataset used to benchmark the model excludes darker skin tones).\n",
    "\n",
    "\n",
    "# Your turn\n",
    "\n",
    "In the exercise, you will **[work directly with a model](https://www.kaggle.com/kernels/fork/15622654)** trained on real-world, biased data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f77425",
   "metadata": {
    "papermill": {
     "duration": 0.00244,
     "end_time": "2024-04-25T18:14:58.884041",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.881601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MLワークフローのさまざまな段階で発生する、これらのさまざまな種類のバイアスを視覚的に表すことができます。\n",
    "\n",
    "[![バイアスのタイプの視覚的表現](https://storage.googleapis.com/kaggle-media/learn/images/bvArGRY.png)](https://arxiv.org/pdf/1901.10002.pdf)\n",
    "\n",
    "これらは相互に排他的ではないことに注意してください。つまり、MLアプリケーションは簡単に複数の種類のバイアスを受ける可能性があります。例えば、Rachel Thomas氏が[最近の研究講演](https://www.youtube.com/watch?v=1Uyc9SPeYkA&list=PLe6zdIMe5B7IR0oDOobXBDBlYY1eqLYPx&index=10&t=41s)で説明しているように、ウェアラブルフィットネスデバイスのMLアプリケーションには次のような問題があります。\n",
    "- **表現バイアス**(モデルのトレーニングに使用されたデータセットで暗い肌の色が除外されている場合) 。\n",
    "− **測定バイアス**(測定装置が暗い肌色で性能低下を示す場合)、および\n",
    "評価バイアス(モデルのベンチマークに使用されたデータセットで暗い肌色が除外されている場合) 。\n",
    "\n",
    "\n",
    "# あなたの番\n",
    "\n",
    "この演習では、**[モデルを直接操作する](https://www.kaggle.com/kernels/fork/15622654)**実世界のバイアスデータでトレーニングします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afcf63",
   "metadata": {
    "papermill": {
     "duration": 0.002757,
     "end_time": "2024-04-25T18:14:58.889506",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.886749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/ai-ethics/discussion) to chat with other learners.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda18a7",
   "metadata": {
    "papermill": {
     "duration": 0.002521,
     "end_time": "2024-04-25T18:14:58.895220",
     "exception": false,
     "start_time": "2024-04-25T18:14:58.892699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*ご質問やご意見がありますか?[Course Discussion Forum](https://www.kaggle.com/learn/ai-ethics/discussion)にアクセスして、他の学習者とチャットしてください。*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.875807,
   "end_time": "2024-04-25T18:14:59.318400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-25T18:14:55.442593",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
